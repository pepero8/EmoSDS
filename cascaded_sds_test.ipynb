{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cascaded SDS 시스템 테스트 입니다.\n",
    "Whisper - GPT2 - googleTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting gtts\n",
      "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting soundfile\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting librosa\n",
      "  Using cached librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jhwan98/anaconda3/envs/emosds/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: setuptools in /home/jhwan98/anaconda3/envs/emosds/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jhwan98/anaconda3/envs/emosds/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting click<8.2,>=7.1 (from gtts)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting CFFI>=1.0 (from sounddevice)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting scipy>=1.2.0 (from librosa)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting scikit-learn>=0.20.0 (from librosa)\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting joblib>=0.14 (from librosa)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/jhwan98/anaconda3/envs/emosds/lib/python3.12/site-packages (from librosa) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.60.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting lazy-loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting pycparser (from CFFI>=1.0->sounddevice)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.43.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/jhwan98/anaconda3/envs/emosds/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.20.0->librosa)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
      "Downloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
      "Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "Downloading pygame-2.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading cffi-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n",
      "Downloading numba-0.60.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.5.0.post1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (248 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading llvmlite-0.43.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: mpmath, urllib3, tqdm, threadpoolctl, sympy, safetensors, regex, pyyaml, pygame, pycparser, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, msgpack, MarkupSafe, llvmlite, lazy-loader, joblib, idna, fsspec, filelock, click, charset-normalizer, certifi, audioread, triton, soxr, scipy, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, jinja2, CFFI, soundfile, sounddevice, scikit-learn, pooch, nvidia-cusolver-cu12, huggingface-hub, gtts, torch, tokenizers, librosa, transformers\n",
      "Successfully installed CFFI-1.17.1 MarkupSafe-3.0.2 audioread-3.0.1 certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 filelock-3.16.1 fsspec-2024.10.0 gtts-2.5.4 huggingface-hub-0.26.2 idna-3.10 jinja2-3.1.4 joblib-1.4.2 lazy-loader-0.4 librosa-0.10.2.post1 llvmlite-0.43.0 mpmath-1.3.0 msgpack-1.1.0 networkx-3.4.2 numba-0.60.0 numpy-2.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pooch-1.8.2 pycparser-2.22 pygame-2.6.1 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sounddevice-0.5.1 soundfile-0.12.1 soxr-0.5.0.post1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.20.3 torch-2.5.1 tqdm-4.67.0 transformers-4.46.3 triton-3.1.0 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers gtts sounddevice soundfile numpy pygame librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from gtts import gTTS\n",
    "# import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "# import pygame\n",
    "import time\n",
    "import os\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechAIAssistant:\n",
    "    def __init__(self, force_cpu=False):\n",
    "        print(\"Initializing Speech AI Assistant...\")\n",
    "\n",
    "        # Set up device\n",
    "        self.device = self._setup_device(force_cpu)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Initialize Whisper for STT\n",
    "        print(\"Loading Whisper model...\")\n",
    "        self.whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "        self.whisper_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "            \"openai/whisper-base\"\n",
    "        )\n",
    "        self.whisper_model = self.whisper_model.to(self.device)\n",
    "\n",
    "        # Initialize GPT-2 for text generation\n",
    "        print(\"Loading GPT-2 model...\")\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.gpt2_model = self.gpt2_model.to(self.device)\n",
    "\n",
    "        # Initialize pygame for audio playback\n",
    "        # pygame.mixer.init()\n",
    "\n",
    "        # Print model memory usage if using CUDA\n",
    "        if self.device.type == \"cuda\":\n",
    "            self._print_gpu_memory_usage()\n",
    "\n",
    "        print(\"Initialization complete!\")\n",
    "\n",
    "    def _setup_device(self, force_cpu):\n",
    "        \"\"\"Setup the device (CPU/GPU) for model inference\"\"\"\n",
    "        if force_cpu:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            # Get the GPU with the most free memory\n",
    "            gpu_id = 0\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                free_memory = []\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    torch.cuda.set_device(i)\n",
    "                    torch.cuda.empty_cache()\n",
    "                    free_memory.append(\n",
    "                        torch.cuda.get_device_properties(i).total_memory\n",
    "                        - torch.cuda.memory_allocated(i)\n",
    "                    )\n",
    "                gpu_id = free_memory.index(max(free_memory))\n",
    "\n",
    "            device = torch.device(f\"cuda:{gpu_id}\")\n",
    "            print(\n",
    "                f\"Found {torch.cuda.device_count()} GPU(s), using GPU {gpu_id}: {torch.cuda.get_device_name(gpu_id)}\"\n",
    "            )\n",
    "            return device\n",
    "        else:\n",
    "            print(\"No GPU found, using CPU\")\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    def _print_gpu_memory_usage(self):\n",
    "        \"\"\"Print current GPU memory usage\"\"\"\n",
    "        if self.device.type == \"cuda\":\n",
    "            print(\"\\nGPU Memory Usage:\")\n",
    "            print(\n",
    "                f\"Allocated: {torch.cuda.memory_allocated(self.device) / 1024**2:.2f} MB\"\n",
    "            )\n",
    "            print(f\"Cached: {torch.cuda.memory_reserved(self.device) / 1024**2:.2f} MB\")\n",
    "\n",
    "    def read_audio_file(self, file_path, target_sr=16000):\n",
    "        \"\"\"\n",
    "        Read audio file and preprocess it for the Whisper model\n",
    "        Supports various audio formats (wav, mp3, etc.)\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the audio file\n",
    "            target_sr (int): Target sampling rate (Whisper expects 16kHz)\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Processed audio array\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio file and resample if necessary\n",
    "            print(f\"Reading audio file: {file_path}\")\n",
    "            audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "\n",
    "            # Convert to mono if stereo\n",
    "            if len(audio.shape) > 1:\n",
    "                audio = librosa.to_mono(audio)\n",
    "\n",
    "            # Normalize audio\n",
    "            audio = librosa.util.normalize(audio)\n",
    "\n",
    "            print(\n",
    "                f\"Successfully loaded audio file: duration = {len(audio)/target_sr:.2f}s\"\n",
    "            )\n",
    "            return audio\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading audio file: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_audio_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Process an audio file through the entire pipeline\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the audio file\n",
    "\n",
    "        Returns:\n",
    "            tuple: (original_text, assistant_response)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read and process audio file\n",
    "            audio = self.read_audio_file(file_path)\n",
    "\n",
    "            # Convert speech to text\n",
    "            text = self.speech_to_text(audio)\n",
    "            print(f\"Transcription: [{text}]\")\n",
    "\n",
    "            # Generate response\n",
    "            response = self.generate_response(text)\n",
    "            print(f\"Assistant: [{response}]\")\n",
    "\n",
    "            # Convert response to speech\n",
    "            self.text_to_speech(response)\n",
    "\n",
    "            return text, response\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio file: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # def record_audio(self, duration=5, sample_rate=16000):\n",
    "    #     \"\"\"Record audio from microphone\"\"\"\n",
    "    #     print(\"Recording... Speak now!\")\n",
    "    #     recording = sd.rec(\n",
    "    #         int(duration * sample_rate), samplerate=sample_rate, channels=1\n",
    "    #     )\n",
    "    #     sd.wait()\n",
    "    #     print(\"Recording complete!\")\n",
    "    #     return recording\n",
    "\n",
    "    def speech_to_text(self, audio):\n",
    "        \"\"\"Convert speech to text using Whisper\"\"\"\n",
    "        # Convert the numpy array to the correct format and move to device\n",
    "        input_features = self.whisper_processor(\n",
    "            audio.squeeze(), sampling_rate=16000, return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "\n",
    "        input_features = input_features.to(self.device)\n",
    "\n",
    "        # Generate token ids\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = self.whisper_model.generate(input_features)\n",
    "\n",
    "        # Decode token ids to text\n",
    "        transcription = self.whisper_processor.batch_decode(\n",
    "            predicted_ids, skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        return transcription\n",
    "\n",
    "    def generate_response(self, text):\n",
    "        \"\"\"Generate response using GPT-2\"\"\"\n",
    "        # Encode the input text and move to device\n",
    "        inputs = self.gpt2_tokenizer.encode(\n",
    "            \"me:\" + text + \" Response:\", return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.gpt2_model.generate(\n",
    "                inputs,\n",
    "                max_length=100,\n",
    "                num_return_sequences=1,\n",
    "                no_repeat_ngram_size=2,\n",
    "                temperature=0.5,\n",
    "            )\n",
    "\n",
    "        response = self.gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Extract only the assistant's response\n",
    "        response = response.split(\"Response:\")[-1].strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "    def text_to_speech(self, text):\n",
    "        \"\"\"Convert text to speech using gTTS\"\"\"\n",
    "        # Generate speech\n",
    "        tts = gTTS(text=text, lang=\"en\")\n",
    "\n",
    "        # Save to temporary file\n",
    "        temp_file = \"temp_speech.mp3\"\n",
    "        tts.save(temp_file)\n",
    "\n",
    "        # Play the audio\n",
    "        # pygame.mixer.music.load(temp_file)\n",
    "        # pygame.mixer.music.play()\n",
    "\n",
    "        # # Wait for audio to finish\n",
    "        # while pygame.mixer.music.get_busy():\n",
    "        #     time.sleep(0.1)\n",
    "\n",
    "        # # Clean up\n",
    "        # pygame.mixer.music.unload()\n",
    "        # os.remove(temp_file)\n",
    "\n",
    "    def process_single_interaction(self):\n",
    "        \"\"\"Process a single interaction with the assistant\"\"\"\n",
    "        # Record audio\n",
    "        audio = self.record_audio()\n",
    "\n",
    "        # Convert speech to text\n",
    "        text = self.speech_to_text(audio)\n",
    "        print(f\"You said: {text}\")\n",
    "\n",
    "        # Generate response\n",
    "        response = self.generate_response(text)\n",
    "        print(f\"Assistant: {response}\")\n",
    "\n",
    "        # Convert response to speech\n",
    "        self.text_to_speech(response)\n",
    "\n",
    "        return text, response\n",
    "\n",
    "    def start_conversation(self, num_turns=3):\n",
    "        \"\"\"Start a conversation with the specified number of turns\"\"\"\n",
    "        print(\"Starting conversation...\")\n",
    "        for i in range(num_turns):\n",
    "            print(f\"\\nTurn {i+1}/{num_turns}\")\n",
    "            self.process_single_interaction()\n",
    "        print(\"\\nConversation ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Speech AI Assistant...\n",
      "Found 3 GPU(s), using GPU 0: NVIDIA A100 80GB PCIe\n",
      "Using device: cuda:0\n",
      "Loading Whisper model...\n",
      "Loading GPT-2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhwan98/anaconda3/envs/emosds/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU Memory Usage:\n",
      "Allocated: 774.22 MB\n",
      "Cached: 814.00 MB\n",
      "Initialization complete!\n",
      "\n",
      "GPU Memory Usage:\n",
      "Allocated: 774.22 MB\n",
      "Cached: 814.00 MB\n",
      "\n",
      "Testing with audio file...\n",
      "Reading audio file: assets/00006.wav\n",
      "Successfully loaded audio file: duration = 4.96s\n",
      "Transcription: [ It's also, I mean that helps a lot with the scenes because you're very much alive]\n",
      "Assistant: [I think it's a little bit of a relief to have that. I'm not sure if it helps you in the sense that you don't have to be a hero to get what you want. But I do think that it makes you feel more alive. And I don' think you have a problem with that, because it doesn't feel like you've been killed. It feels]\n",
      "\n",
      "GPU Memory Usage:\n",
      "Allocated: 774.22 MB\n",
      "Cached: 878.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Create assistant instance with optional GPU selection\n",
    "assistant = SpeechAIAssistant(force_cpu=False)  # Set to True to force CPU usage\n",
    "\n",
    "try:\n",
    "    # Optional: Monitor GPU memory before processing\n",
    "    if assistant.device.type == \"cuda\":\n",
    "        assistant._print_gpu_memory_usage()\n",
    "\n",
    "    # Test with an audio file\n",
    "    print(\"\\nTesting with audio file...\")\n",
    "    test_file = \"assets/00006.wav\"  # Replace with your audio file path\n",
    "    text, response = assistant.process_audio_file(test_file)\n",
    "    assistant.text_to_speech(response)\n",
    "\n",
    "    # Optional: Monitor GPU memory after processing\n",
    "    if assistant.device.type == \"cuda\":\n",
    "        assistant._print_gpu_memory_usage()\n",
    "\n",
    "    # print(\"\\nStarting live conversation...\")\n",
    "    # # Start conversation with 3 turns\n",
    "    # assistant.start_conversation(num_turns=3)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nConversation interrupted by user.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "finally:\n",
    "    # Clean up GPU memory\n",
    "    if assistant.device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emosds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
